{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFTqR_sd3ZDV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "bc9d2d20-0f18-4606-dc8f-2658322c8b4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0033b278727e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train FastText model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfasttext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "\n",
        "# Train FastText model\n",
        "sentences = [text.split() for text in X_train]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=2, sg=1)\n",
        "\n",
        "# Function to get FastText embeddings\n",
        "def get_fasttext_vectors(text, model, vector_size):\n",
        "    tokens = text.split()\n",
        "    vector = np.zeros(vector_size)\n",
        "    valid_tokens = 0\n",
        "    for token in tokens:\n",
        "        if token in model.wv:\n",
        "            vector += model.wv[token]\n",
        "            valid_tokens += 1\n",
        "    return vector / valid_tokens if valid_tokens > 0 else vector\n",
        "\n",
        "# Convert training and test sets to FastText embeddings\n",
        "X_train_fasttext = np.array([get_fasttext_vectors(text, fasttext_model, 100) for text in X_train])\n",
        "X_test_fasttext = np.array([get_fasttext_vectors(text, fasttext_model, 100) for text in X_test])\n",
        "\n",
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "\n",
        "# Train FastText model\n",
        "sentences = [text.split() for text in X_train]\n",
        "fasttext_model = FastText(sentences, vector_size=100, window=5, min_count=2, sg=1)\n",
        "\n",
        "# Function to get FastText embeddings\n",
        "def get_fasttext_vectors(text, model, vector_size):\n",
        "    tokens = text.split()\n",
        "    vector = np.zeros(vector_size)\n",
        "    valid_tokens = 0\n",
        "    for token in tokens:\n",
        "        if token in model.wv:\n",
        "            vector += model.wv[token]\n",
        "            valid_tokens += 1\n",
        "    return vector / valid_tokens if valid_tokens > 0 else vector\n",
        "\n",
        "# Convert training and test sets to FastText embeddings\n",
        "X_train_fasttext = np.array([get_fasttext_vectors(text, fasttext_model, 100) for text in X_train])\n",
        "X_test_fasttext = np.array([get_fasttext_vectors(text, fasttext_model, 100) for text in X_test])\n",
        "\n",
        "from gensim.downloader import load\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained GloVe model\n",
        "glove_model = load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Function to get GloVe embeddings\n",
        "def get_glove_vectors(text, model, vector_size):\n",
        "    tokens = text.split()\n",
        "    vector = np.zeros(vector_size)\n",
        "    valid_tokens = 0\n",
        "    for token in tokens:\n",
        "        if token in model:\n",
        "            vector += model[token]\n",
        "            valid_tokens += 1\n",
        "    return vector / valid_tokens if valid_tokens > 0 else vector\n",
        "\n",
        "# Convert training and test sets to GloVe embeddings\n",
        "X_train_glove = np.array([get_glove_vectors(text, glove_model, 100) for text in X_train])\n",
        "X_test_glove = np.array([get_glove_vectors(text, glove_model, 100) for text in X_test])\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Initialize BERT model and tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_vectors(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Convert training and test sets to BERT embeddings\n",
        "X_train_bert = np.array([get_bert_vectors(text, bert_tokenizer, bert_model) for text in X_train])\n",
        "X_test_bert = np.array([get_bert_vectors(text, bert_tokenizer, bert_model) for text in X_test])"
      ]
    }
  ]
}