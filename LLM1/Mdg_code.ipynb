{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0TbhHQAnjjW"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "distilled_student_sentiment_classifier = pipeline(\n",
        "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
        "    return_all_scores=True\n",
        ")"
      ],
      "metadata": {
        "id": "Qx8kz369nnPz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English text\n",
        "english_text = \"I love this movie and I would watch it again and again!\"\n",
        "english_result = distilled_student_sentiment_classifier(english_text)\n",
        "\n",
        "# Malay text\n",
        "malay_text = \"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\"\n",
        "malay_result = distilled_student_sentiment_classifier(malay_text)\n",
        "\n",
        "# Japanese text\n",
        "japanese_text = \"私はこの映画が大好きで、何度も見ます！\"\n",
        "japanese_result = distilled_student_sentiment_classifier(japanese_text)\n",
        "\n",
        "# Print the results\n",
        "print(\"English sentiment analysis result:\")\n",
        "print(english_result)\n",
        "\n",
        "print(\"\\nMalay sentiment analysis result:\")\n",
        "print(malay_result)\n",
        "\n",
        "print(\"\\nJapanese sentiment analysis result:\")\n",
        "print(japanese_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtIVfwiMrI8v",
        "outputId": "4a3244e3-525a-4d1e-fb27-7bf8a6311675"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.9754056930541992}, {'label': 'neutral', 'score': 0.01555436011403799}, {'label': 'negative', 'score': 0.009039935655891895}]]\n",
            "\n",
            "Malay sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.9760094285011292}, {'label': 'neutral', 'score': 0.018045149743556976}, {'label': 'negative', 'score': 0.0059454599395394325}]]\n",
            "\n",
            "Japanese sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.9342429041862488}, {'label': 'neutral', 'score': 0.040193185210227966}, {'label': 'negative', 'score': 0.0255639236420393}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
        "import torch\n",
        "\n",
        "# Step 3: Create the manual dataset\n",
        "texts = [\n",
        "    \"I love this movie and I would watch it again and again!\",\n",
        "    \"This film was terrible, I couldn't even finish it.\",\n",
        "    \"Absolutely fantastic experience, highly recommend!\",\n",
        "    \"Not worth the time, very boring and slow.\",\n",
        "    \"The plot was interesting and the acting was great.\",\n",
        "    \"Horrible movie, will never watch it again.\",\n",
        "    \"A masterpiece, beautifully executed.\",\n",
        "    \"Pretty average, nothing special.\",\n",
        "    \"Terrible plot and bad acting, do not recommend.\",\n",
        "    \"Enjoyed every moment of it, a must-watch!\"\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n",
        "\n",
        "# Convert the texts and labels into a format suitable for fine-tuning\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Step 4: Initialize tokenizer and create dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
        "dataset = SimpleDataset(texts, labels, tokenizer)\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "\n",
        "# Step 5: Fine-Tune the Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", num_labels=3)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Use the Fine-Tuned Model for Sentiment Analysis\n",
        "fine_tuned_sentiment_classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "# Analyze sentiments in different languages\n",
        "texts_to_analyze = [\n",
        "    (\"English\", \"I love this movie and I would watch it again and again!\"),\n",
        "    (\"Malay\", \"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\"),\n",
        "    (\"Japanese\", \"私はこの映画が大好きで、何度も見ます！\")\n",
        "]\n",
        "\n",
        "for language, text in texts_to_analyze:\n",
        "    result = fine_tuned_sentiment_classifier(text)\n",
        "    print(f\"\\n{language} sentiment analysis result:\")\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "LEohPsfOskbA",
        "outputId": "3c340a0d-b4ad-4074-cb4d-2f6492209e0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.156434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.795476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.690278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "English sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.7180215120315552}, {'label': 'neutral', 'score': 0.17993322014808655}, {'label': 'negative', 'score': 0.10204528272151947}]]\n",
            "\n",
            "Malay sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.8495468497276306}, {'label': 'neutral', 'score': 0.11804182827472687}, {'label': 'negative', 'score': 0.03241141512989998}]]\n",
            "\n",
            "Japanese sentiment analysis result:\n",
            "[[{'label': 'positive', 'score': 0.7794512510299683}, {'label': 'neutral', 'score': 0.14067824184894562}, {'label': 'negative', 'score': 0.07987045496702194}]]\n"
          ]
        }
      ]
    }
  ]
}